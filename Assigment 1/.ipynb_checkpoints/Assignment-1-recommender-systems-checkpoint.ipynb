{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "This notebook contains the first part of assignment 1 about recommender systems.\n",
    "\n",
    "By:  \n",
    "Antoni Czernek (S4000595) (a.a.czernek@umail.leidenuniv.nl)  \n",
    "Art Schenkel (S3745244) (j.a.schenkel@umail.leidenuniv.nl)  \n",
    "Sadaf Esmaeili Rad (S3986160) (sadafismaeili@gmail.com)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dependencies and reading in the data\n",
    "The first cell of this notebook is used for importing the needed dependencies. The following cells read in the data from the MovieLens 1M dataset. We will mainly use the rating_df for learning and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('ml-1m/movies.dat',\n",
    "                        delimiter='::', engine= 'python', header=None,\n",
    "                        names=['Movie_Id', 'movie_name', 'genre'], encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = pd.read_csv('ml-1m/ratings.dat',\n",
    "                        delimiter='::', engine= 'python', header=None,\n",
    "                        names=['user_id', 'Movie_Id','Ratings','Time_stamp'], encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_csv('ml-1m/users.dat',\n",
    "                        delimiter='::', header=None,\n",
    "                        names=['user_id', 'Gender','Age','Occupation','Zip-Code'], encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "The first part of the code implements the cross-validation. The cross-validation funtion return a dataframe with a randomly assigned number of folds. This dataframe is then used to learn and test the models, by choosing one of the fold as a valid test, and the other as the train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(df,n_folds):\n",
    "    shuffled_df = df.sample(random_state = 42, frac =1)\n",
    "    shuffled_df['Fold']= None\n",
    "    shuffled_df.reset_index(inplace = True)\n",
    "    shuffled_df.drop(columns = 'index', inplace = True)\n",
    "    data_size = len(shuffled_df)\n",
    "    for i in range(1,n_folds):\n",
    "        shuffled_df.loc[int((i-1)/n_folds*data_size):int(i/n_folds * data_size),'Fold'] = i\n",
    "    shuffled_df.loc[int((n_folds-1)/n_folds*data_size):,'Fold']= n_folds\n",
    "    return shuffled_df\n",
    "\n",
    "data = cross_validation(rating_df, 5)\n",
    "data.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Approaches\n",
    "We will now start with the naive approaches. These are: global average (ratingGlobal()), movie average (ratingItem()), user average (ratingUser()) and a linear combination of the three averages (ratingUserItem() and ratingUserStarItem). We also calculate the optimal parameters for linear regression by applying the least-squares solution algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global rating: 3.581564453029317\n"
     ]
    }
   ],
   "source": [
    "# rating global, return mean of all ratings\n",
    "def ratingGlobal():\n",
    "    return rating_df[\"Ratings\"].mean()\n",
    "\n",
    "print(\"Global rating:\" , ratingGlobal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item rating: 4.390724637681159\n"
     ]
    }
   ],
   "source": [
    "# rating item, return mean of all ratings for a specific item\n",
    "def ratingItem(item):\n",
    "    join = pd.merge(movies_df, rating_df, how='left', on=\"Movie_Id\")\n",
    "    result = join[join[\"Movie_Id\"] == item]\n",
    "    return result[\"Ratings\"].mean()\n",
    "\n",
    "print(\"Item rating:\", ratingItem(1193))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Rating: 3.9019607843137254\n"
     ]
    }
   ],
   "source": [
    "# rating user, return mean of all rating for a specific user\n",
    "def ratingUser(user):\n",
    "    join = pd.merge(users_df, rating_df, how='left', on=\"user_id\")\n",
    "    result = join[join[\"user_id\"] == user]\n",
    "    return result[\"Ratings\"].mean()\n",
    "\n",
    "print(\"User Rating:\", ratingUser(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Item Rating: 3.300204867377632\n",
      "actual rating: 3\n"
     ]
    }
   ],
   "source": [
    "# rating user item, combines user rating and item rating multiplied by paramer alpha and beta respectively. Lastly parameter gamma is added\n",
    "def ratingUserItem(user, item, alpha, beta, gamma):\n",
    "    result = alpha * ratingUser(user) + beta * ratingItem(item) + gamma\n",
    "\n",
    "    # make sure the result is a valid rating, between 1 and 5\n",
    "    if(result > 5): result = 5\n",
    "    if(result < 1): result = 1\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"User Item Rating:\", ratingUserItem(1, 1193, 0.41, 0.34, 0.09))\n",
    "print(\"actual rating:\", rating(1, 661))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User* Item Rating: 3.826720575022462\n"
     ]
    }
   ],
   "source": [
    "# rating user star item is similar to rating user item, but wwithout adding parameter gamma at the end\n",
    "def ratingUserStarItem(user, item, alpha, beta):\n",
    "    result = alpha * ratingUser(user) + beta * ratingItem(item)\n",
    "\n",
    "    # make sure the result is a valid rating, between 1 and 5\n",
    "    if(result > 5): result = 5\n",
    "    if(result < 1): result = 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"User* Item Rating:\", ratingUserStarItem(1, 661, 0.5, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# This helper function returns a rating given a user_id and movie_id\n",
    "def rating(user, item):\n",
    "    result1 = rating_df[rating_df[\"user_id\"] == user]\n",
    "    result2 = result1[result1[\"Movie_Id\"] == item]\n",
    "    return int(result2.Ratings)\n",
    "\n",
    "print(rating(1, 661))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal values:\n",
      "Alpha: 0.4113321969726794\n",
      "Beta: 0.34024284095705803\n",
      "Gamma: 0.09820092990789195\n"
     ]
    }
   ],
   "source": [
    "# This function calculates the optimal value for parameters alpha, beta and gamma for a specific user and movie using linear regression.\n",
    "def linearRegression(user, item):\n",
    "    avguser = np.array([ratingUser(user)])\n",
    "    avgmovie = np.array([ratingItem(item)])\n",
    "    currrating = np.array([rating(user, item)])\n",
    "\n",
    "    a = np.column_stack((avguser, avgmovie, np.ones_like(avguser)))\n",
    "    b = currrating\n",
    "\n",
    "    x, residuals, rank, s = np.linalg.lstsq(a, b, rcond=None)\n",
    "\n",
    "    alpha, beta, gamma = x\n",
    "\n",
    "    print(\"Optimal values:\")\n",
    "    print(\"Alpha:\", alpha)\n",
    "    print(\"Beta:\", beta)\n",
    "    print(\"Gamma:\", gamma)\n",
    "\n",
    "linearRegression(1,661)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UV Matrix Decomposition\n",
    "Next, we implemented UV matrix decomposition as described in section 9.4 of the MMDS textbook. \n",
    "\n",
    "As in the Matrix Factorization algorithm implemented next, we use dictionaries to translate the movie_id and user_id to matrix_id\n",
    "\n",
    "In this algorithm in case of a movie_id (or user_id) appearing in the test example, but not present in the train set we simply use the average of the whole set as the prediction. A different approach is used in the Matrix Factorization part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uvMatrixDecomp():\n",
    "    # Load the rating data\n",
    "    DR = rating_df\n",
    "\n",
    "    # Set hyperparameters and the Kfold implemented in sklearn\n",
    "    kf = KFold(n_splits = 5 , shuffle = True)\n",
    "    c = 2 \n",
    "    i = 5 # Number of iterations\n",
    "\n",
    "    test_results_rmse =[]\n",
    "    test_results_mae =[]\n",
    "    # Split the data into training and testing sets\n",
    "    for train_index , test_index in kf.split(DR):\n",
    "        DR_train, DR_test = DR.loc[train_index], DR.loc[test_index]\n",
    "\n",
    "        Row = DR_train.pivot(index = 'user_id', columns ='Movie_Id', values = 'Ratings')\n",
    "\n",
    "        #Creating a dictionaries for the \n",
    "        values_user = np.unique(DR_train['user_id'])\n",
    "        user_dict = {values_user[idxx] : idxx for idxx in range(len(values_user))}\n",
    "        values_movie = np.unique(DR_train['Movie_Id'])\n",
    "        movie_dict = {values_movie[idxx] : idxx for idxx in range(len(values_movie))}\n",
    "\n",
    "        # Calculating the means and creating the matrix that we will then learn on\n",
    "        # that is, the matrix of user_id and movie_id as rows and columns\n",
    "        u_mean = Row.mean(axis=1)\n",
    "        Row_array = Row.to_numpy()\n",
    "        u_mean = u_mean.to_numpy()\n",
    "\n",
    "        normal = Row_array # - u_mean.reshape(-1,1)\n",
    "        N = normal\n",
    "\n",
    "        #initializing the values of the U and V vectors, as simply just ones\n",
    "        u = np.full((normal.shape[0],2), 1)\n",
    "        v = np.full((2,normal.shape[1]), 1)\n",
    "        u = u.astype(np.float32)\n",
    "        v = v.astype(np.float32)\n",
    "\n",
    "        #calculating the UV matrix\n",
    "        uv = np.dot(u,v)\n",
    "\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "        # Perform matrix factorization\n",
    "        for iterations in range(i):\n",
    "            # Updating the rows of the U matrix\n",
    "            for r in range(Row_array.shape[0]):\n",
    "\n",
    "                for s in range(c):\n",
    "                    sums = 0\n",
    "                    u_rk = u[r,:]\n",
    "                    v_kj = v[:,:]\n",
    "                    \n",
    "                    # This part below is used to calculate the Prj element, as it uses the sum\n",
    "                    # over U and V without the s row\n",
    "                    u_rk_del = np.delete(u_rk, s, 0)\n",
    "                    v_kj_del = np.delete(v_kj, s, 0)\n",
    "                    v_sj = v[s,:]\n",
    "                    v_sj_squared = v_sj ** 2\n",
    "\n",
    "                    u_rk_v_kj = np.dot(u_rk_del, v_kj_del)\n",
    "                    m_rj = N[r,:]\n",
    "\n",
    "                    error = m_rj - u_rk_v_kj\n",
    "\n",
    "                    # calculating the new value for the U[r,s]\n",
    "                    vsj_dot_er = v_sj * error\n",
    "                    sums = np.nansum(vsj_dot_er)\n",
    "                    v_sj_ssum = np.nansum((v_sj_squared) * (~np.isnan(m_rj)))\n",
    "                    newval_u = sums / v_sj_ssum\n",
    "                    u[r,s] = u[r,s] + ((newval_u - u[r,s]))\n",
    "            \n",
    "            # Below is the update part for the V matrix, which is very similar to the U update\n",
    "            for r in range(c):\n",
    "                for s in range(Row_array.shape[1]):\n",
    "                    sums = 0\n",
    "                \n",
    "                    u_ik = u[:,:]\n",
    "                    v_ks = v[:,s]\n",
    "                    u_ik_del = np.delete(u_ik, r, 1)\n",
    "\n",
    "                    v_ks_del = np.delete(v_ks, r, 0)\n",
    "                    u_ir = u[:,r]\n",
    "                    u_ir_squared = u_ir ** 2\n",
    "\n",
    "                    u_ik_v_ks = np.dot(u_ik_del, v_ks_del)\n",
    "                    m_is = N[:,s]\n",
    "                    error = m_is - u_ik_v_ks\n",
    "\n",
    "                    uir_dot_er = u_ir * error\n",
    "                    sumsv = np.nansum(uir_dot_er)\n",
    "                    u_ir_ssum = np.nansum(u_ir_squared * (~np.isnan(m_is)))\n",
    "                    newval_v = sumsv / u_ir_ssum\n",
    "                    v[r,s] = v[r,s] + ((newval_v - v[r,s]))\n",
    "\n",
    "            # Calculate and show the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE)\n",
    "            uv = np.dot(u,v)\n",
    "            dif = uv -normal\n",
    "\n",
    "            print(\"Iteration Number: \",iterations )\n",
    "\n",
    "            dif_abs= (np.absolute(dif))\n",
    "            dif_abs_0s = np.nan_to_num(dif_abs)\n",
    "            dif_abs_sum = np.sum(dif_abs_0s,axis=0)\n",
    "            sum_dif = dif_abs_sum.sum()\n",
    "            non_0_count = np.count_nonzero(dif_abs_0s)\n",
    "            MAE=sum_dif/non_0_count\n",
    "\n",
    "            # print('MAE',MAE)\n",
    "\n",
    "            dif_sqr = dif ** 2\n",
    "            dif_sqr_0s = np.nan_to_num(dif_sqr)\n",
    "            dif_sqr_total= np.sum( dif_sqr_0s ,axis=0)\n",
    "            sumz = dif_sqr_total.sum()\n",
    "            non_0_count_sqr = np.count_nonzero( dif_sqr_0s )\n",
    "            RME = sumz/ non_0_count_sqr\n",
    "            rme_list=[RME]\n",
    "\n",
    "            # print('RMSE=',RME)\n",
    "\n",
    "        # Testing part, we get the prediction matrix\n",
    "        pred_matrix = np.dot(u,v)\n",
    "        preds = []\n",
    "\n",
    "        #We calculate the number of failed examples - examples where either movie_id or user_id\n",
    "        # does not appear in the train set but appears in the test_set\n",
    "        fails = 0\n",
    "        for idx in range(len(DR_test)):\n",
    "            try:\n",
    "                prediction = pred_matrix[user_dict[DR_test.iloc[idx]['user_id']]][movie_dict[DR_test.iloc[idx]['Movie_Id']]]\n",
    "            except:\n",
    "                # For the mismatching of the user_id or movie_id we just use the global average rating\n",
    "                fails += 1\n",
    "                prediction = 3.58\n",
    "            preds.append(prediction)\n",
    "        labels = DR_test['Ratings']\n",
    "        print(f'% failure: {fails/len(DR_test)}')\n",
    "        test_rmse =  mean_squared_error(labels,np.array(preds), squared = False)\n",
    "        test_mae = mean_absolute_error(labels,np.array(preds))\n",
    "        test_results_rmse.append(test_rmse)\n",
    "        test_results_mae.append(test_mae)\n",
    "        print(test_rmse)\n",
    "    print('Mean results from K-Fold:')\n",
    "    print(f'RMSE: {np.mean(np.array(test_results_rmse))} \\nMAE: {np.mean(np.array(test_results_mae))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [      1       4       7 ... 1000206 1000207 1000208] TEST: [      0       2       3 ... 1000201 1000202 1000205]\n",
      "Iteration Number:  0\n",
      "Iteration Number:  1\n",
      "Iteration Number:  2\n",
      "Iteration Number:  3\n",
      "Iteration Number:  4\n",
      "% failure: 0.00020995590925905559\n",
      "0.9124043462221751\n",
      "TRAIN: [      0       2       3 ... 1000206 1000207 1000208] TEST: [      1       8      23 ... 1000185 1000191 1000196]\n",
      "Iteration Number:  0\n",
      "Iteration Number:  1\n",
      "Iteration Number:  2\n",
      "Iteration Number:  3\n",
      "Iteration Number:  4\n",
      "% failure: 0.00015996640705451856\n",
      "0.9112103994020335\n",
      "TRAIN: [      0       1       2 ... 1000204 1000205 1000208] TEST: [      9      15      20 ... 1000198 1000206 1000207]\n",
      "Iteration Number:  0\n",
      "Iteration Number:  1\n",
      "Iteration Number:  2\n",
      "Iteration Number:  3\n",
      "Iteration Number:  4\n",
      "% failure: 0.00013497165595225002\n",
      "0.9077844690729202\n",
      "TRAIN: [      0       1       2 ... 1000205 1000206 1000207] TEST: [      7      11      12 ... 1000192 1000195 1000208]\n",
      "Iteration Number:  0\n",
      "Iteration Number:  1\n",
      "Iteration Number:  2\n",
      "Iteration Number:  3\n",
      "Iteration Number:  4\n",
      "% failure: 0.00018996010837724078\n",
      "0.9148217965570179\n",
      "TRAIN: [      0       1       2 ... 1000206 1000207 1000208] TEST: [      4      13      14 ... 1000200 1000203 1000204]\n",
      "Iteration Number:  0\n",
      "Iteration Number:  1\n",
      "Iteration Number:  2\n",
      "Iteration Number:  3\n",
      "Iteration Number:  4\n",
      "% failure: 0.00015496823151253992\n",
      "0.9110027985192626\n",
      "Mean results from K-Fold:\n",
      "RMSE: 0.9114447619546819 \n",
      "MAE: 0.7203229435519438\n"
     ]
    }
   ],
   "source": [
    "# perform the UV matrix decomposition\n",
    "uvMatrixDecomp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization\n",
    "Lastly we implemented matrix factorization as described in the gravity-Tikk paper. We create the model as two matrixes of features for the users and movies. Sizes of the matrixes are determined by the unique number of user_id (accordingly movie_id for the item matrix) and the number of features that is chosen when creating the model. \n",
    "\n",
    "As the train set may include gaps in the numbering of either user_id or movie_id we use dictionaries to translate the movie_id (or user_id) to matrix_id\n",
    "\n",
    "For the learning part, we use the algorithm described on the slides from the lecture, we calculate the prediction as a scalar multiplication of the according vectors in matrixes of users and items. Then we calculate the error and update the vectors by learning_rate*(error*user_matrix_vector- lamb*self.item_matrix_vector). As we can see there are two parameters learning rate and lambda that we can tune.\n",
    "\n",
    "We initialize the matrixes with random numbers using np.random.rand function\n",
    "\n",
    "The output is put into [1:5] interval by simply setting every output < 1 into 1 and every output > 5 as 5, other outputs are not changed.\n",
    "\n",
    "In the testing function, when the test example uses user_id that did not appear in the train set (in case of a movie_id not appearing in the train set proceed similarly) we take the sum of the vectors for the corresponding movie_id and scale the sums of features of all movie_ids into a [1:5] interval, and read the value of the movie_id that is asked in the testing example (after the scaling). \n",
    "\n",
    "The assumption is that the bigger the sum of features the bigger the chance is for the movie to get a high score. This assumption is not 100% true, as for example having feature < 0 with a corresponding <0 feature in the user_id vector creates a positive result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization:\n",
    "    def __init__(self,x, num_features):\n",
    "        #initilaze two matrixes that then multiply by each other to give a matrix of ratings\n",
    "        \n",
    "        user_size = np.unique(x['user_id']).shape[0]\n",
    "        item_size = np.unique(x['Movie_Id']).shape[0]\n",
    "\n",
    "        values_user = np.unique(x['user_id'])\n",
    "        self.dict_user = {values_user[i] : i for i in range(len(values_user))}\n",
    "\n",
    "        values_item = np.unique(x['Movie_Id'])\n",
    "        self.dict_item = {values_item[i] : i for i in range(len(values_item))}\n",
    "        \n",
    "        self.user_matrix = np.random.rand(user_size,num_features)\n",
    "        self.item_matrix = np.random.rand(item_size,num_features)\n",
    "        \n",
    "    def fit(self,x, learning_rate = 0.005, lamb = 0.05, n_iter = 10):\n",
    "        for it in range(n_iter):\n",
    "            tmp = 0\n",
    "            for i in range(len(x)):\n",
    "                user = x.loc[i]['user_id']\n",
    "                item = x.loc[i]['Movie_Id']\n",
    "\n",
    "                user_idx = self.dict_user[user]\n",
    "                item_idx = self.dict_item[item]\n",
    "                \n",
    "                #calculate the error\n",
    "                error = x.loc[i]['Ratings'] - min(max(np.matmul(self.user_matrix[user_idx],self.item_matrix[item_idx]),1),5)\n",
    "                # update values\n",
    "                self.user_matrix[user_idx] = self.user_matrix[user_idx] + learning_rate*(error*self.item_matrix[item_idx] - lamb*self.user_matrix[user_idx])\n",
    "                self.item_matrix[item_idx] = self.item_matrix[item_idx] + learning_rate*(error*self.user_matrix[user_idx] - lamb*self.item_matrix[item_idx])\n",
    "\n",
    "                tmp += 1\n",
    "                if tmp%50000 ==0:\n",
    "                    print(f'currently done: {tmp/len(x)} % of the iteration {it}')\n",
    "\n",
    "        \n",
    "        print('current iteration ended: '+str(it))\n",
    "    def test(self,x):\n",
    "        predictions = []\n",
    "        for i in range(len(x)):\n",
    "            \n",
    "            user = x.loc[i]['user_id']\n",
    "            item = x.loc[i]['Movie_Id']\n",
    "\n",
    "            try:\n",
    "                user_idx = self.dict_user[user]\n",
    "                item_idx = self.dict_item[item]\n",
    "                pred = min(max(np.matmul(self.user_matrix[user_idx],self.item_matrix[item_idx]),1),5)\n",
    "                predictions.append(pred)\n",
    "                \n",
    "            except: #If there is no user\n",
    "                try:\n",
    "                    item_idx = self.dict_item[item]\n",
    "                    sum_item = np.sum(self.item_matrix[item_idx])\n",
    "                    sums = np.sum(self.item_matrix, axis = 1)\n",
    "                    pred =  (sum_item- np.min(sums)) / (np.max(sums) - np.min(sums)) * (4) + 1\n",
    "                    predictions.append(pred)\n",
    "                \n",
    "                except: # If there is no movie\n",
    "                    user_idx = self.dict_user[user]\n",
    "                    sum_user= np.sum(self.user_matrix[item_idx])\n",
    "                    sums = np.sum(self.user_matrix, axis = 1)\n",
    "                    pred =  (sum_user - np.min(sums)) / (np.max(sums) - np.min(sums)) * (4) + 1\n",
    "                    predictions.append(pred)\n",
    "                #calculate the error\n",
    "            \n",
    "        labels = np.array(x['Ratings'])\n",
    "        predictions = np.array(predictions)\n",
    "        rmse =  mean_squared_error(labels,predictions, squared = False)\n",
    "        mse = mean_absolute_error(labels,predictions)\n",
    "\n",
    "        return rmse, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the results we managed to get using this method with different parameters. The best results we got from the parameters:\n",
    "\n",
    "n_features = 10 ; n_iter = 15 ; learning_rate = 0.01 ; lambda = 0.03\n",
    "\n",
    "comment - We have noticed an error in the implementation of the MatrixFactorization that the number of unique movie_id was taken from the dataset itself, so the matrix had all of them implemented (at random of course). The learning on these sets underneath is on the wrongly implemented constructor. We believe that this error would not change the results by a lot. Unfortunately, as the function fit time takes hours, especially with that number of iterations, and the error was found on the day of the deadline we couldn't refit the functions. We provide the old implementation under the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_list = []\n",
    "mae_list = []\n",
    "for i in range(1,6):\n",
    "    train = data.loc[data['Fold'] != i ]\n",
    "    train = train.reset_index()\n",
    "    valid = data.loc[data['Fold'] == i ] \n",
    "    valid = valid.reset_index()\n",
    "    mt = MatrixFactorization(train,50)\n",
    "    print('Fitting the fold: ' + str(i))\n",
    "\n",
    "    mt.fit(train, n_iter = 10)\n",
    "    rmse, mae = mt.test(valid)\n",
    "    print(f\"Fold {i} RMSE: {rmse} \\n MSE: {mae}\")\n",
    "    \n",
    "    rmse_list.append(rmse)\n",
    "    mae_list.append(mae)\n",
    "print(f'Mean results:\\nRMSE: {np.mean(np.array(rmse_list))} \\nMAE: {np.mean(np.array(mae_list))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the results so they don't have to be recalculated each time\n",
    "np.save('user_matrix_2.npy',mt.user_matrix)\n",
    "np.save('item_matrix_2.npy',mt.item_matrix)\n",
    "\n",
    "list_user = list(mt.dict_user.items())\n",
    "np.save('dict_user_2.npy',np.array(list_user))\n",
    "\n",
    "key_list_item = list(mt.dict_item.keys())\n",
    "item_list_item = list(mt.dict_item.items())\n",
    "np.save('dict_items_2.npy',np.array(item_list_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_list = []\n",
    "mae_list = []\n",
    "for i in range(1,6):\n",
    "    train = data.loc[data['Fold'] != i ]\n",
    "    train = train.reset_index()\n",
    "    valid = data.loc[data['Fold'] == i ] \n",
    "    valid = valid.reset_index()\n",
    "    mt_2 = MatrixFactorization(train,20)\n",
    "    print('Fitting the fold: ' + str(i))\n",
    "\n",
    "    mt_2.fit(train,learning_rate = 0.01, lamb = 0.03, n_iter = 15)\n",
    "    rmse, mae = mt_2.test(valid)\n",
    "    print(f\"Fold {i} RMSE: {rmse} \\n MSE: {mae}\")\n",
    "    \n",
    "    rmse_list.append(rmse)\n",
    "    mae_list.append(mae)\n",
    "print(f'Mean results:\\nRMSE: {np.mean(np.array(rmse_list))} \\nMAE: {np.mean(np.array(mae_list))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_rmse = 10000\n",
    "user_params = None\n",
    "item_params = None\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "for i in range(1,6):\n",
    "    train = data.loc[data['Fold'] != i ]\n",
    "    train = train.reset_index()\n",
    "    valid = data.loc[data['Fold'] == i ] \n",
    "    valid = valid.reset_index()\n",
    "    mt_3 = MatrixFactorization(train,20)\n",
    "    print('Fitting the fold: ' + str(i))\n",
    "\n",
    "    mt_3.fit(train,learning_rate = 0.002, lamb = 0.04, n_iter = 10)\n",
    "    rmse, mae = mt_3.test(valid)\n",
    "    print(f\"Fold {i} RMSE: {rmse} \\n MSE: {mae}\")\n",
    "    \n",
    "    rmse_list.append(rmse)\n",
    "    mae_list.append(mae)\n",
    "print(f'Mean results:\\nRMSE: {np.mean(np.array(rmse_list))} \\nMAE: {np.mean(np.array(mae_list))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('user_matrix.npy',mt_2.user_matrix)\n",
    "np.save('item_matrix.npy',mt_2.item_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_2.dict_user\n",
    "list_user = list(mt_2.dict_user.items())\n",
    "np.save('dict_user.npy',np.array(list_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_2.dict_item\n",
    "key_list_item = list(mt_2.dict_item.keys())\n",
    "item_list_item = list(mt_2.dict_item.items())\n",
    "np.save('dict_items.npy',np.array(item_list_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion part one\n",
    "This concludes the first part of our report about recommender systems. For the data visualisation of the results of our matrix factorization method, see part two of the report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
