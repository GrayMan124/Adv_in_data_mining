{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "This notebook contains the first part of assignment 1 about recommender systems.\n",
    "\n",
    "By:  \n",
    "Antoni Czernek (S4000595) (a.a.czernek@umail.leidenuniv.nl)  \n",
    "Art Schenkel (S3745244) (j.a.schenkel@umail.leidenuniv.nl)  \n",
    "Sadaf Esmaeili Rad (S3986160) (sadafismaeili@gmail.com)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dependencies and reading in the data\n",
    "The first cell of this notebook is used for importing the needed dependencies. The following cells read in the data from the MovieLens 1M dataset. We will mainly use the rating_df for learning and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('ml-1m/movies.dat',\n",
    "                        delimiter='::', engine= 'python', header=None,\n",
    "                        names=['Movie_Id', 'movie_name', 'genre'], encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = pd.read_csv('ml-1m/ratings.dat',\n",
    "                        delimiter='::', engine= 'python', header=None,\n",
    "                        names=['user_id', 'Movie_Id','Ratings','Time_stamp'], encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_csv('ml-1m/users.dat',\n",
    "                        delimiter='::', header=None,\n",
    "                        names=['user_id', 'Gender','Age','Occupation','Zip-Code'], encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "The first part of the code implements the cross-validation. The cross-validation funtion return a dataframe with a randomly assigned number of folds. This dataframe is then used to learn and test the models, by choosing one of the fold as a valid test, and the other as the train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(df,n_folds):\n",
    "    shuffled_df = df.sample(random_state = 42, frac =1)\n",
    "    shuffled_df['Fold']= None\n",
    "    shuffled_df.reset_index(inplace = True)\n",
    "    shuffled_df.drop(columns = 'index', inplace = True)\n",
    "    data_size = len(shuffled_df)\n",
    "    for i in range(1,n_folds):\n",
    "        shuffled_df.loc[int((i-1)/n_folds*data_size):int(i/n_folds * data_size),'Fold'] = i\n",
    "    shuffled_df.loc[int((n_folds-1)/n_folds*data_size):,'Fold']= n_folds\n",
    "    return shuffled_df\n",
    "\n",
    "data = cross_validation(rating_df, 5)\n",
    "data.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Approaches\n",
    "We will now start with the naive approaches. These are: global average (ratingGlobal()), movie average (ratingItem()), user average (ratingUser()) and a linear combination of the three averages (ratingUserItem() and ratingUserStarItem). We also calculate the optimal parameters for linear regression by applying the least-squares solution algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global rating: 3.581564453029317\n"
     ]
    }
   ],
   "source": [
    "# rating global, return mean of all ratings\n",
    "def ratingGlobal():\n",
    "    return rating_df[\"Ratings\"].mean()\n",
    "\n",
    "print(\"Global rating:\" , ratingGlobal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item rating: 4.390724637681159\n"
     ]
    }
   ],
   "source": [
    "# rating item, return mean of all ratings for a specific item\n",
    "def ratingItem(item):\n",
    "    join = pd.merge(movies_df, rating_df, how='left', on=\"Movie_Id\")\n",
    "    result = join[join[\"Movie_Id\"] == item]\n",
    "    return result[\"Ratings\"].mean()\n",
    "\n",
    "print(\"Item rating:\", ratingItem(1193))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Rating: 3.9019607843137254\n"
     ]
    }
   ],
   "source": [
    "# rating user, return mean of all rating for a specific user\n",
    "def ratingUser(user):\n",
    "    join = pd.merge(users_df, rating_df, how='left', on=\"user_id\")\n",
    "    result = join[join[\"user_id\"] == user]\n",
    "    return result[\"Ratings\"].mean()\n",
    "\n",
    "print(\"User Rating:\", ratingUser(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Item Rating: 3.300204867377632\n",
      "actual rating: 3\n"
     ]
    }
   ],
   "source": [
    "# rating user item, combines user rating and item rating multiplied by paramer alpha and beta respectively. Lastly parameter gamma is added\n",
    "def ratingUserItem(user, item, alpha, beta, gamma):\n",
    "    result = alpha * ratingUser(user) + beta * ratingItem(item) + gamma\n",
    "\n",
    "    # make sure the result is a valid rating, between 1 and 5\n",
    "    if(result > 5): result = 5\n",
    "    if(result < 1): result = 1\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"User Item Rating:\", ratingUserItem(1, 1193, 0.41, 0.34, 0.09))\n",
    "print(\"actual rating:\", rating(1, 661))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User* Item Rating: 3.826720575022462\n"
     ]
    }
   ],
   "source": [
    "# rating user star item is similar to rating user item, but wwithout adding parameter gamma at the end\n",
    "def ratingUserStarItem(user, item, alpha, beta):\n",
    "    result = alpha * ratingUser(user) + beta * ratingItem(item)\n",
    "\n",
    "    # make sure the result is a valid rating, between 1 and 5\n",
    "    if(result > 5): result = 5\n",
    "    if(result < 1): result = 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"User* Item Rating:\", ratingUserStarItem(1, 661, 0.5, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# This helper function returns a rating given a user_id and movie_id\n",
    "def rating(user, item):\n",
    "    result1 = rating_df[rating_df[\"user_id\"] == user]\n",
    "    result2 = result1[result1[\"Movie_Id\"] == item]\n",
    "    return int(result2.Ratings)\n",
    "\n",
    "print(rating(1, 661))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal values:\n",
      "Alpha: 0.4113321969726794\n",
      "Beta: 0.34024284095705803\n",
      "Gamma: 0.09820092990789195\n"
     ]
    }
   ],
   "source": [
    "# This function calculates the optimal value for parameters alpha, beta and gamma for a specific user and movie using linear regression.\n",
    "def linearRegression(user, item):\n",
    "    avguser = np.array([ratingUser(user)])\n",
    "    avgmovie = np.array([ratingItem(item)])\n",
    "    currrating = np.array([rating(user, item)])\n",
    "\n",
    "    a = np.column_stack((avguser, avgmovie, np.ones_like(avguser)))\n",
    "    b = currrating\n",
    "\n",
    "    x, residuals, rank, s = np.linalg.lstsq(a, b, rcond=None)\n",
    "\n",
    "    alpha, beta, gamma = x\n",
    "\n",
    "    print(\"Optimal values:\")\n",
    "    print(\"Alpha:\", alpha)\n",
    "    print(\"Beta:\", beta)\n",
    "    print(\"Gamma:\", gamma)\n",
    "\n",
    "linearRegression(1,661)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UV Matrix Decomposition\n",
    "Next, we implemented UV matrix decomposition as described in section 9.4 of the MMDS textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uvMatrixDecomp():\n",
    "    # Load the rating data\n",
    "    DR = rating_df\n",
    "\n",
    "    # Set hyperparameters\n",
    "    kf = KFold(n_splits = 5 , shuffle = True)\n",
    "    c = 2\n",
    "    i = 5 # Number of iterations\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    for train_index , test_index in kf.split(DR):\n",
    "        DR_train, DR_test = DR.loc[train_index], DR.loc[test_index]\n",
    "\n",
    "        Row = DR_train.pivot(index = 'user_id', columns ='Movie_Id', values = 'Ratings')\n",
    "\n",
    "        u_mean = Row.mean(axis=1)\n",
    "        Row_array = Row.to_numpy()\n",
    "        u_mean = u_mean.to_numpy()\n",
    "\n",
    "        normal = Row_array - u_mean.reshape(-1,1)\n",
    "        N = normal\n",
    "\n",
    "        u = np.full((normal.shape[0],2), 1)\n",
    "        v = np.full((2,normal.shape[1]), 1)\n",
    "        u = u.astype(np.float32)\n",
    "        v = v.astype(np.float32)\n",
    "        uv = np.dot(u,v)\n",
    "\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "        # Perform matrix factorization\n",
    "        for iterations in range(i):\n",
    "            for r in range(940):\n",
    "\n",
    "                for s in range(c):\n",
    "                    sums = 0\n",
    "                    u_rk = u[r,:]\n",
    "                    v_kj = v[:,:]\n",
    "                    u_rk_del = np.delete(u_rk, s, 0)\n",
    "                    v_kj_del = np.delete(v_kj, s, 0)\n",
    "                    v_sj = v[s,:]\n",
    "                    v_sj_squared = v_sj ** 2\n",
    "\n",
    "                    u_rk_v_kj = np.dot(u_rk_del, v_kj_del)\n",
    "                    m_rj = N[r,:]\n",
    "\n",
    "                    error = m_rj - u_rk_v_kj\n",
    "\n",
    "                    vsj_dot_er = v_sj * error\n",
    "                    sums = np.nansum(vsj_dot_er)\n",
    "                    v_sj_ssum = np.nansum((v_sj_squared) * (~np.isnan(m_rj)))\n",
    "                    newval_u = sums / v_sj_ssum\n",
    "                    u[r,s] = u[r,s] + ((newval_u - u[r,s]))\n",
    "\n",
    "            for r in range(c):\n",
    "                for s in range(Row_array.shape[1]):\n",
    "                    sums = 0\n",
    "                \n",
    "                    u_ik = u[:,:]\n",
    "                    v_ks = v[:,s]\n",
    "                    u_ik_del = np.delete(u_ik, r, 1)\n",
    "\n",
    "                    v_ks_del = np.delete(v_ks, r, 0)\n",
    "                    u_ir = u[:,r]\n",
    "                    u_ir_squared = u_ir ** 2\n",
    "\n",
    "                    u_ik_v_ks = np.dot(u_ik_del, v_ks_del)\n",
    "                    m_is = N[:,s]\n",
    "                    error = m_is - u_ik_v_ks\n",
    "\n",
    "                    uir_dot_er = u_ir * error\n",
    "                    sumsv = np.nansum(uir_dot_er)\n",
    "                    u_ir_ssum = np.nansum(u_ir_squared * (~np.isnan(m_is)))\n",
    "                    newval_v = sumsv / u_ir_ssum\n",
    "                    v[r,s] = v[r,s] + ((newval_v - v[r,s]))\n",
    "\n",
    "            # Calculate and show the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE)\n",
    "            uv = np.dot(u,v)\n",
    "            dif = uv -normal\n",
    "\n",
    "            print(\"Iteration Number: \",iterations )\n",
    "\n",
    "            dif_abs= (np.absolute(dif))\n",
    "            dif_abs_0s = np.nan_to_num(dif_abs)\n",
    "            dif_abs_sum = np.sum(dif_abs_0s,axis=0)\n",
    "            sum_dif = dif_abs_sum.sum()\n",
    "            non_0_count = np.count_nonzero(dif_abs_0s)\n",
    "            MAE=sum_dif/non_0_count\n",
    "\n",
    "            print('MAE',MAE)\n",
    "\n",
    "            dif_sqr = dif ** 2\n",
    "            dif_sqr_0s = np.nan_to_num(dif_sqr)\n",
    "            dif_sqr_total= np.sum( dif_sqr_0s ,axis=0)\n",
    "            sumz = dif_sqr_total.sum()\n",
    "            non_0_count_sqr = np.count_nonzero( dif_sqr_0s )\n",
    "            RME = sumz/ non_0_count_sqr\n",
    "            rme_list=[RME]\n",
    "\n",
    "            print('RMSE=',RME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [      1       2       3 ... 1000205 1000206 1000207] TEST: [      0       8      30 ... 1000197 1000198 1000208]\n",
      "Iteration Number:  0\n",
      "MAE 0.8297612195845893\n",
      "RMSE= 1.1308829614244478\n",
      "Iteration Number:  1\n",
      "MAE 0.7226590517128179\n",
      "RMSE= 0.839567427033562\n",
      "Iteration Number:  2\n",
      "MAE 0.7200137545356283\n",
      "RMSE= 0.8334217506994597\n",
      "Iteration Number:  3\n",
      "MAE 0.7196269012568751\n",
      "RMSE= 0.8325779239060479\n",
      "Iteration Number:  4\n",
      "MAE 0.7194628736522254\n",
      "RMSE= 0.832221222405838\n",
      "TRAIN: [      0       1       4 ... 1000206 1000207 1000208] TEST: [      2       3       6 ... 1000196 1000201 1000203]\n",
      "Iteration Number:  0\n",
      "MAE 0.8301445753097326\n",
      "RMSE= 1.1306074584026997\n",
      "Iteration Number:  1\n",
      "MAE 0.7231677207518704\n",
      "RMSE= 0.8397588187747302\n",
      "Iteration Number:  2\n",
      "MAE 0.7205655845805726\n",
      "RMSE= 0.8336777826007961\n",
      "Iteration Number:  3\n",
      "MAE 0.7201919920038254\n",
      "RMSE= 0.8328785882670897\n",
      "Iteration Number:  4\n",
      "MAE 0.7200400046320701\n",
      "RMSE= 0.832553584931888\n",
      "TRAIN: [      0       1       2 ... 1000206 1000207 1000208] TEST: [     10      13      20 ... 1000191 1000202 1000205]\n",
      "Iteration Number:  0\n",
      "MAE 0.8303033589694309\n",
      "RMSE= 1.1313122023555542\n",
      "Iteration Number:  1\n",
      "MAE 0.7233059103402285\n",
      "RMSE= 0.840424962005381\n",
      "Iteration Number:  2\n",
      "MAE 0.7206740347301765\n",
      "RMSE= 0.834288351712949\n",
      "Iteration Number:  3\n",
      "MAE 0.7202670843708735\n",
      "RMSE= 0.8334422913469016\n",
      "Iteration Number:  4\n",
      "MAE 0.7200996542931433\n",
      "RMSE= 0.8330943683206736\n",
      "TRAIN: [      0       2       3 ... 1000205 1000206 1000208] TEST: [      1       7       9 ... 1000199 1000200 1000207]\n",
      "Iteration Number:  0\n",
      "MAE 0.8298271974527189\n",
      "RMSE= 1.1302925744584584\n",
      "Iteration Number:  1\n",
      "MAE 0.7229459596038901\n",
      "RMSE= 0.8396798959281355\n",
      "Iteration Number:  2\n",
      "MAE 0.7203344580612483\n",
      "RMSE= 0.833611989508166\n",
      "Iteration Number:  3\n",
      "MAE 0.7199345313581275\n",
      "RMSE= 0.8327812476441542\n",
      "Iteration Number:  4\n",
      "MAE 0.7197665075306905\n",
      "RMSE= 0.8324394798883038\n",
      "TRAIN: [      0       1       2 ... 1000205 1000207 1000208] TEST: [      4       5      14 ... 1000187 1000204 1000206]\n",
      "Iteration Number:  0\n",
      "MAE 0.8299825510107464\n",
      "RMSE= 1.1301268928977182\n",
      "Iteration Number:  1\n",
      "MAE 0.7230578584913067\n",
      "RMSE= 0.8394321257049344\n",
      "Iteration Number:  2\n",
      "MAE 0.7204427448373929\n",
      "RMSE= 0.8333855399778675\n",
      "Iteration Number:  3\n",
      "MAE 0.720063357684856\n",
      "RMSE= 0.8325663774032085\n",
      "Iteration Number:  4\n",
      "MAE 0.7199041072275579\n",
      "RMSE= 0.8322206850570142\n"
     ]
    }
   ],
   "source": [
    "# perform the UV matrix decomposition\n",
    "uvMatrixDecomp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization\n",
    "Lastly we implemented matrix factorization as described in the gravity-Tikk paper. We create the model as two matrixes of features for the users and movies. Sizes of the matrixes are determined by the unique number of user_id (accordingly movie_id for the item matrix) and the number of features that is chosen when creating the model. \n",
    "\n",
    "As the train set may include gaps in the numbering of either user_id or movie_id we use dictionaries to translate the movie_id (or user_id) to matrix_id\n",
    "\n",
    "For the learning part, we use the algorithm described on the slides from the lecture, we calculate the prediction as a scalar multiplication of the according vectors in matrixes of users and items. Then we calculate the error and update the vectors by learning_rate*(error*user_matrix_vector- lamb*self.item_matrix_vector). As we can see there are two parameters learning rate and lambda that we can tune.\n",
    "\n",
    "We initialize the matrixes with random numbers using np.random.rand function\n",
    "\n",
    "The output is put into [1:5] interval by simply setting every output < 1 into 1 and every output > 5 as 5, other outputs are not changed.\n",
    "\n",
    "In the testing function, when the test example uses user_id that did not appear in the train set (in case of a movie_id not appearing in the train set proceed similarly) we take the sum of the vectors for the corresponding movie_id and scale the sums of features of all movie_ids into a [1:5] interval, and read the value of the movie_id that is asked in the testing example (after the scaling). \n",
    "\n",
    "The assumption is that the bigger the sum of features the bigger the chance is for the movie to get a high score. This assumption is not 100% true, as for example having feature < 0 with a corresponding <0 feature in the user_id vector creates a positive result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization:\n",
    "    def __init__(self,x, num_features):\n",
    "        #initilaze two matrixes that then multiply by each other to give a matrix of ratings\n",
    "        \n",
    "        user_size = np.unique(x['user_id']).shape[0]\n",
    "        item_size = np.unique(x['Movie_Id']).shape[0]\n",
    "\n",
    "        values_user = np.unique(x['user_id'])\n",
    "        self.dict_user = {values_user[i] : i for i in range(len(values_user))}\n",
    "\n",
    "        values_item = np.unique(x['Movie_Id'])\n",
    "        self.dict_item = {values_item[i] : i for i in range(len(values_item))}\n",
    "        \n",
    "        self.user_matrix = np.random.rand(user_size,num_features)\n",
    "        self.item_matrix = np.random.rand(item_size,num_features)\n",
    "        \n",
    "    def fit(self,x, learning_rate = 0.005, lamb = 0.05, n_iter = 10):\n",
    "        for it in range(n_iter):\n",
    "            tmp = 0\n",
    "            for i in range(len(x)):\n",
    "                user = x.loc[i]['user_id']\n",
    "                item = x.loc[i]['Movie_Id']\n",
    "\n",
    "                user_idx = self.dict_user[user]\n",
    "                item_idx = self.dict_item[item]\n",
    "                \n",
    "                #calculate the error\n",
    "                error = x.loc[i]['Ratings'] - min(max(np.matmul(self.user_matrix[user_idx],self.item_matrix[item_idx]),1),5)\n",
    "                # update values\n",
    "                self.user_matrix[user_idx] = self.user_matrix[user_idx] + learning_rate*(error*self.item_matrix[item_idx] - lamb*self.user_matrix[user_idx])\n",
    "                self.item_matrix[item_idx] = self.item_matrix[item_idx] + learning_rate*(error*self.user_matrix[user_idx] - lamb*self.item_matrix[item_idx])\n",
    "\n",
    "                tmp += 1\n",
    "                if tmp%50000 ==0:\n",
    "                    print(f'currently done: {tmp/len(x)} % of the iteration {it}')\n",
    "\n",
    "        \n",
    "        print('current iteration ended: '+str(it))\n",
    "    def test(self,x):\n",
    "        predictions = []\n",
    "        for i in range(len(x)):\n",
    "            \n",
    "            user = x.loc[i]['user_id']\n",
    "            item = x.loc[i]['Movie_Id']\n",
    "\n",
    "            try:\n",
    "                user_idx = self.dict_user[user]\n",
    "                item_idx = self.dict_item[item]\n",
    "                pred = min(max(np.matmul(self.user_matrix[user_idx],self.item_matrix[item_idx]),1),5)\n",
    "                predictions.append(pred)\n",
    "                \n",
    "            except: #If there is no user\n",
    "                try:\n",
    "                    item_idx = self.dict_item[item]\n",
    "                    sum_item = np.sum(self.item_matrix[item_idx])\n",
    "                    sums = np.sum(self.item_matrix, axis = 1)\n",
    "                    pred =  (sum_item- np.min(sums)) / (np.max(sums) - np.min(sums)) * (4) + 1\n",
    "                    predictions.append(pred)\n",
    "                \n",
    "                except: # If there is no movie\n",
    "                    user_idx = self.dict_user[user]\n",
    "                    sum_user= np.sum(self.user_matrix[item_idx])\n",
    "                    sums = np.sum(self.user_matrix, axis = 1)\n",
    "                    pred =  (sum_user - np.min(sums)) / (np.max(sums) - np.min(sums)) * (4) + 1\n",
    "                    predictions.append(pred)\n",
    "                #calculate the error\n",
    "            \n",
    "        labels = np.array(x['Ratings'])\n",
    "        predictions = np.array(predictions)\n",
    "        rmse =  mean_squared_error(labels,predictions, squared = False)\n",
    "        mse = mean_absolute_error(labels,predictions)\n",
    "\n",
    "        return rmse, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the results we managed to get using this method with different parameters. The best results we got from the parameters:\n",
    "\n",
    "n_features = 10 ; n_iter = 15 ; learning_rate = 0.01 ; lambda = 0.03\n",
    "\n",
    "comment - We have noticed an error in the implementation of the MatrixFactorization that the number of unique movie_id was taken from the dataset itself, so the matrix had all of them implemented (at random of course). The learning on these sets underneath is on the wrongly implemented constructor. We believe that this error would not change the results by a lot. Unfortunately, as the function fit time takes hours, especially with that number of iterations, and the error was found on the day of the deadline we couldn't refit the functions. We provide the old implementation under the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_list = []\n",
    "mae_list = []\n",
    "for i in range(1,6):\n",
    "    train = data.loc[data['Fold'] != i ]\n",
    "    train = train.reset_index()\n",
    "    valid = data.loc[data['Fold'] == i ] \n",
    "    valid = valid.reset_index()\n",
    "    mt = MatrixFactorization(train,50)\n",
    "    print('Fitting the fold: ' + str(i))\n",
    "\n",
    "    mt.fit(train, n_iter = 10)\n",
    "    rmse, mae = mt.test(valid)\n",
    "    print(f\"Fold {i} RMSE: {rmse} \\n MSE: {mae}\")\n",
    "    \n",
    "    rmse_list.append(rmse)\n",
    "    mae_list.append(mae)\n",
    "print(f'Mean results:\\nRMSE: {np.mean(np.array(rmse_list))} \\nMAE: {np.mean(np.array(mae_list))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the results so they don't have to be recalculated each time\n",
    "np.save('user_matrix_2.npy',mt.user_matrix)\n",
    "np.save('item_matrix_2.npy',mt.item_matrix)\n",
    "\n",
    "list_user = list(mt.dict_user.items())\n",
    "np.save('dict_user_2.npy',np.array(list_user))\n",
    "\n",
    "key_list_item = list(mt.dict_item.keys())\n",
    "item_list_item = list(mt.dict_item.items())\n",
    "np.save('dict_items_2.npy',np.array(item_list_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_list = []\n",
    "mae_list = []\n",
    "for i in range(1,6):\n",
    "    train = data.loc[data['Fold'] != i ]\n",
    "    train = train.reset_index()\n",
    "    valid = data.loc[data['Fold'] == i ] \n",
    "    valid = valid.reset_index()\n",
    "    mt_2 = MatrixFactorization(train,20)\n",
    "    print('Fitting the fold: ' + str(i))\n",
    "\n",
    "    mt_2.fit(train,learning_rate = 0.01, lamb = 0.03, n_iter = 15)\n",
    "    rmse, mae = mt_2.test(valid)\n",
    "    print(f\"Fold {i} RMSE: {rmse} \\n MSE: {mae}\")\n",
    "    \n",
    "    rmse_list.append(rmse)\n",
    "    mae_list.append(mae)\n",
    "print(f'Mean results:\\nRMSE: {np.mean(np.array(rmse_list))} \\nMAE: {np.mean(np.array(mae_list))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_rmse = 10000\n",
    "user_params = None\n",
    "item_params = None\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "for i in range(1,6):\n",
    "    train = data.loc[data['Fold'] != i ]\n",
    "    train = train.reset_index()\n",
    "    valid = data.loc[data['Fold'] == i ] \n",
    "    valid = valid.reset_index()\n",
    "    mt_3 = MatrixFactorization(train,20)\n",
    "    print('Fitting the fold: ' + str(i))\n",
    "\n",
    "    mt_3.fit(train,learning_rate = 0.002, lamb = 0.04, n_iter = 10)\n",
    "    rmse, mae = mt_3.test(valid)\n",
    "    print(f\"Fold {i} RMSE: {rmse} \\n MSE: {mae}\")\n",
    "    \n",
    "    rmse_list.append(rmse)\n",
    "    mae_list.append(mae)\n",
    "print(f'Mean results:\\nRMSE: {np.mean(np.array(rmse_list))} \\nMAE: {np.mean(np.array(mae_list))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('user_matrix.npy',mt_2.user_matrix)\n",
    "np.save('item_matrix.npy',mt_2.item_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_2.dict_user\n",
    "list_user = list(mt_2.dict_user.items())\n",
    "np.save('dict_user.npy',np.array(list_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_2.dict_item\n",
    "key_list_item = list(mt_2.dict_item.keys())\n",
    "item_list_item = list(mt_2.dict_item.items())\n",
    "np.save('dict_items.npy',np.array(item_list_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion part one\n",
    "This concludes the first part of our report about recommender systems. For the data visualisation of the results of our matrix factorization method, see part two of the report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
