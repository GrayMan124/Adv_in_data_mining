{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "This notebook contains the first part of assignment 1 about recommender systems.\n",
    "\n",
    "By:\n",
    "Antoni\n",
    "Art Schenkel (3745244) (j.a.schenkel@umail.leidenuniv.nl)\n",
    "Sadaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data, we will mainly use the rating_df for learning and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('ml-1m/movies.dat',\n",
    "                        delimiter='::', engine= 'python', header=None,\n",
    "                        names=['Movie_Id', 'movie_name', 'genre'], encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = pd.read_csv('ml-1m/ratings.dat',\n",
    "                        delimiter='::', engine= 'python', header=None,\n",
    "                        names=['user_id', 'Movie_Id','Ratings','Time_stamp'], encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Art\\AppData\\Local\\Temp\\ipykernel_17188\\3957394905.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  users_df = pd.read_csv('ml-1m/users.dat',\n"
     ]
    }
   ],
   "source": [
    "users_df = pd.read_csv('ml-1m/users.dat',\n",
    "                        delimiter='::', header=None,\n",
    "                        names=['user_id', 'Gender','Age','Occupation','Zip-Code'], encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "The first part of the code implements the cross-validation. The cross-validation funtion return a dataframe with a randomly assigned number of folds. This dataframe is then used to learn and test the models, by choosing one of the fold as a valid test, and the other as the train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(df,n_folds):\n",
    "    shuffled_df = df.sample(random_state = 42, frac =1)\n",
    "    shuffled_df['Fold']= None\n",
    "    shuffled_df.reset_index(inplace = True)\n",
    "    shuffled_df.drop(columns = 'index', inplace = True)\n",
    "    data_size = len(shuffled_df)\n",
    "    for i in range(1,n_folds):\n",
    "        shuffled_df.loc[int((i-1)/n_folds*data_size):int(i/n_folds * data_size),'Fold'] = i\n",
    "    shuffled_df.loc[int((n_folds-1)/n_folds*data_size):,'Fold']= n_folds\n",
    "    return shuffled_df\n",
    "\n",
    "data = cross_validation(rating_df, 5)\n",
    "data.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Approaches\n",
    "We will now start with the naive approaches. These are: global average (ratingGlobal()), movie average (ratingItem), user average (ratingUser()) and a linear combination of the three averages(ratingUserItem())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global rating: 3.581564453029317\n"
     ]
    }
   ],
   "source": [
    "# rating global, return mean of all ratings\n",
    "def ratingGlobal():\n",
    "    return rating_df[\"Ratings\"].mean()\n",
    "\n",
    "print(\"Global rating:\" , ratingGlobal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item rating: 4.390724637681159\n"
     ]
    }
   ],
   "source": [
    "# rating item, return mean of all ratings for a specific item\n",
    "def ratingItem(item):\n",
    "    join = pd.merge(movies_df, rating_df, how='left', on=\"Movie_Id\")\n",
    "    result = join[join[\"Movie_Id\"] == item]\n",
    "    return result[\"Ratings\"].mean()\n",
    "\n",
    "print(\"Item rating:\", ratingItem(1193))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Rating: 3.9019607843137254\n"
     ]
    }
   ],
   "source": [
    "# rating user, return mean of all rating for a specific user\n",
    "def ratingUser(user):\n",
    "    join = pd.merge(users_df, rating_df, how='left', on=\"user_id\")\n",
    "    result = join[join[\"user_id\"] == user]\n",
    "    return result[\"Ratings\"].mean()\n",
    "\n",
    "print(\"User Rating:\", ratingUser(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Item Rating: 4.389701941482089\n"
     ]
    }
   ],
   "source": [
    "# rating user item, combines user rating and item rating multiplied by paramer alpha and beta respectively. Lastly parameter gamma is added\n",
    "def ratingUserItem(user, item, alpha, beta, gamma):\n",
    "    return alpha * ratingUser(user) + beta * ratingItem(item) + gamma\n",
    "\n",
    "print(\"User Item Rating:\", ratingUserItem(1, 1193, 0.5, 0.5, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User* Item Rating: 4.28970194148209\n"
     ]
    }
   ],
   "source": [
    "# rating user star item is similar to rating user item, but wwithout adding parameter gamma at the end\n",
    "def ratingUserStarItem(user, item, alpha, beta):\n",
    "    return alpha * ratingUser(user) + beta * ratingItem(item)\n",
    "\n",
    "print(\"User* Item Rating:\", ratingUserStarItem(1, 1193, 0.5, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Art\\AppData\\Local\\Temp\\ipykernel_17188\\648485187.py:5: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  return int(result2.Ratings)\n"
     ]
    }
   ],
   "source": [
    "# This helper function returns a rating given a user_id and movie_id\n",
    "def rating(user, item):\n",
    "    result1 = rating_df[rating_df[\"user_id\"] == user]\n",
    "    result2 = result1[result1[\"Movie_Id\"] == item]\n",
    "    return int(result2.Ratings)\n",
    "\n",
    "print(rating(1, 661))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal values:\n",
      "Alpha: 0.4113321969726794\n",
      "Beta: 0.34024284095705803\n",
      "Gamma: 0.09820092990789195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Art\\AppData\\Local\\Temp\\ipykernel_17188\\648485187.py:5: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  return int(result2.Ratings)\n"
     ]
    }
   ],
   "source": [
    "# This function calculates the optimal value for parameters alpha, beta and gamma for a specific user and movie using linear regression.\n",
    "def linearRegression(user, item):\n",
    "    avguser = np.array([ratingUser(user)])\n",
    "    avgmovie = np.array([ratingItem(item)])\n",
    "    currrating = np.array([rating(user, item)])\n",
    "\n",
    "    a = np.column_stack((avguser, avgmovie, np.ones_like(avguser)))\n",
    "    b = currrating\n",
    "\n",
    "    coefficients, residuals, rank, singular_values = np.linalg.lstsq(a, b, rcond=None)\n",
    "\n",
    "    alpha, beta, gamma = coefficients\n",
    "\n",
    "    print(\"Optimal values:\")\n",
    "    print(\"Alpha:\", alpha)\n",
    "    print(\"Beta:\", beta)\n",
    "    print(\"Gamma:\", gamma)\n",
    "\n",
    "linearRegression(1,661)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply linear regression to all data by filling avguser and avgmovie completely. also use fallback rule (global average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UV Matrix Decomposition\n",
    "Next we implemented UV matrix decomposition as described in section 9.4 of the MMDS textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uvMatrixDecomp():\n",
    "    DR = rating_df\n",
    "    kf = KFold(n_splits = 5 , shuffle = True)\n",
    "    c = 2\n",
    "    i = 5\n",
    "    for train_index , test_index in kf.split(DR):\n",
    "        DR_train, DR_test = DR.loc[train_index], DR.loc[test_index]\n",
    "        Row = DR_train.pivot(index = 'user_id', columns ='Movie_Id', values = 'Ratings')\n",
    "        u_mean = Row.mean(axis=1)\n",
    "        Row_array = Row.to_numpy()\n",
    "        u_mean = u_mean.to_numpy()\n",
    "        normal = Row_array - u_mean.reshape(-1,1)\n",
    "        N = normal\n",
    "        u = np.full((normal.shape[0],2), 1)\n",
    "        v = np.full((2,normal.shape[1]), 1)\n",
    "        u = u.astype(np.float32)\n",
    "        v = v.astype(np.float32)\n",
    "        uv = np.dot(u,v)\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        for iterations in range(i):\n",
    "            for r in range(940):\n",
    "\n",
    "                for s in range(c):\n",
    "                    sums = 0\n",
    "                    u_rk = u[r,:]\n",
    "                    v_kj = v[:,:]\n",
    "                    u_rk_del = np.delete(u_rk, s, 0)\n",
    "                    v_kj_del = np.delete(v_kj, s, 0)\n",
    "                    v_sj = v[s,:]\n",
    "                    v_sj_squared = v_sj ** 2\n",
    "\n",
    "                    u_rk_v_kj = np.dot(u_rk_del, v_kj_del)\n",
    "                    m_rj = N[r,:]\n",
    "\n",
    "                    error = m_rj - u_rk_v_kj\n",
    "\n",
    "                    vsj_dot_er = v_sj * error\n",
    "                    sums = np.nansum(vsj_dot_er)\n",
    "                    v_sj_ssum = np.nansum((v_sj_squared) * (~np.isnan(m_rj)))\n",
    "                    newval_u = sums / v_sj_ssum\n",
    "                    u[r,s] = u[r,s] + ((newval_u - u[r,s]))\n",
    "            for r in range(c):\n",
    "                for s in range(Row_array.shape[1]):\n",
    "                    sums = 0\n",
    "                \n",
    "\n",
    "                    u_ik = u[:,:]\n",
    "                    v_ks = v[:,s]\n",
    "                    u_ik_del = np.delete(u_ik, r, 1)\n",
    "\n",
    "                    v_ks_del = np.delete(v_ks, r, 0)\n",
    "                    u_ir = u[:,r]\n",
    "                    u_ir_squared = u_ir ** 2\n",
    "\n",
    "                    u_ik_v_ks = np.dot(u_ik_del, v_ks_del)\n",
    "                    m_is = N[:,s]\n",
    "                    error = m_is - u_ik_v_ks\n",
    "\n",
    "                    uir_dot_er = u_ir * error\n",
    "                    sumsv = np.nansum(uir_dot_er)\n",
    "                    u_ir_ssum = np.nansum(u_ir_squared * (~np.isnan(m_is)))\n",
    "                    newval_v = sumsv / u_ir_ssum\n",
    "                    v[r,s] = v[r,s] + ((newval_v - v[r,s]))\n",
    "            uv = np.dot(u,v)\n",
    "            dif = uv -normal\n",
    "            print(\"Iteration Number: \",iterations )\n",
    "            dif_abs= (np.absolute(dif))\n",
    "            dif_abs_0s = np.nan_to_num(dif_abs)\n",
    "            dif_abs_sum = np.sum(dif_abs_0s,axis=0)\n",
    "            sum_dif = dif_abs_sum.sum()\n",
    "            non_0_count = np.count_nonzero(dif_abs_0s)\n",
    "            MAE=sum_dif/non_0_count\n",
    "            print('MAE',MAE)\n",
    "            dif_sqr = dif ** 2\n",
    "            dif_sqr_0s = np.nan_to_num(dif_sqr)\n",
    "            dif_sqr_total= np.sum( dif_sqr_0s ,axis=0)\n",
    "            sumz = dif_sqr_total.sum()\n",
    "            non_0_count_sqr = np.count_nonzero( dif_sqr_0s )\n",
    "            RME = sumz/ non_0_count_sqr\n",
    "            rme_list=[RME]\n",
    "            print('RMSE=',RME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [      0       1       2 ... 1000206 1000207 1000208] TEST: [     16      19      22 ... 1000187 1000194 1000204]\n",
      "Iteration Number:  0\n",
      "MAE 0.8302802179206353\n",
      "RMSE= 1.1312279668339145\n",
      "Iteration Number:  1\n",
      "MAE 0.7232146960318103\n",
      "RMSE= 0.840447876967069\n",
      "Iteration Number:  2\n",
      "MAE 0.7205743706384972\n",
      "RMSE= 0.8343832578566551\n",
      "Iteration Number:  3\n",
      "MAE 0.7201799563618158\n",
      "RMSE= 0.8335733769097614\n",
      "Iteration Number:  4\n",
      "MAE 0.7200168663897022\n",
      "RMSE= 0.8332424202635018\n",
      "TRAIN: [      0       1       2 ... 1000205 1000207 1000208] TEST: [      4       6       7 ... 1000190 1000193 1000206]\n",
      "Iteration Number:  0\n",
      "MAE 0.8297374488366678\n",
      "RMSE= 1.1300575859451367\n",
      "Iteration Number:  1\n",
      "MAE 0.7227668366681609\n",
      "RMSE= 0.8388680146046142\n",
      "Iteration Number:  2\n",
      "MAE 0.7201789934358878\n",
      "RMSE= 0.8327903835780063\n",
      "Iteration Number:  3\n",
      "MAE 0.7197928845117092\n",
      "RMSE= 0.8319625916509858\n",
      "Iteration Number:  4\n",
      "MAE 0.7196270415512771\n",
      "RMSE= 0.8316070915581323\n",
      "TRAIN: [      0       1       2 ... 1000205 1000206 1000208] TEST: [      5       9      10 ... 1000198 1000201 1000207]\n",
      "Iteration Number:  0\n",
      "MAE 0.82980654892184\n",
      "RMSE= 1.1306587018745915\n",
      "Iteration Number:  1\n",
      "MAE 0.7228665597278521\n",
      "RMSE= 0.8395278993007957\n",
      "Iteration Number:  2\n",
      "MAE 0.7202270602839543\n",
      "RMSE= 0.8333615774201997\n",
      "Iteration Number:  3\n",
      "MAE 0.719837303340503\n",
      "RMSE= 0.8325204917492167\n",
      "Iteration Number:  4\n",
      "MAE 0.7196827677567447\n",
      "RMSE= 0.8321763844197448\n",
      "TRAIN: [      0       2       4 ... 1000205 1000206 1000207] TEST: [      1       3       8 ... 1000200 1000202 1000208]\n",
      "Iteration Number:  0\n",
      "MAE 0.830422820312712\n",
      "RMSE= 1.131131295512782\n",
      "Iteration Number:  1\n",
      "MAE 0.7233420449238117\n",
      "RMSE= 0.8402180021988229\n",
      "Iteration Number:  2\n",
      "MAE 0.7207352086446333\n",
      "RMSE= 0.8341409384025483\n",
      "Iteration Number:  3\n",
      "MAE 0.7203524162526789\n",
      "RMSE= 0.8333415958272682\n",
      "Iteration Number:  4\n",
      "MAE 0.7201940832542869\n",
      "RMSE= 0.8330130318381129\n",
      "TRAIN: [      1       3       4 ... 1000206 1000207 1000208] TEST: [      0       2      12 ... 1000192 1000203 1000205]\n",
      "Iteration Number:  0\n",
      "MAE 0.8298099773733194\n",
      "RMSE= 1.130294642336306\n",
      "Iteration Number:  1\n",
      "MAE 0.7229052043381013\n",
      "RMSE= 0.8397774649840364\n",
      "Iteration Number:  2\n",
      "MAE 0.7202979155179221\n",
      "RMSE= 0.8337058353325121\n",
      "Iteration Number:  3\n",
      "MAE 0.7199084099020244\n",
      "RMSE= 0.8328827930934651\n",
      "Iteration Number:  4\n",
      "MAE 0.71974660716732\n",
      "RMSE= 0.8325437668440482\n"
     ]
    }
   ],
   "source": [
    "# perform the UV matrix decomposition\n",
    "uvMatrixDecomp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization\n",
    "Lastly we implemented matrix factorization as described in the gravity-Tikk paper. Implementing the model, we create the model as two matrixes of features for the users and movies. Sizes of the matrixes are determined by the unique number of user_id (accordingly movie_id for the item matrix) and the number of features that is chosen when creating the model. \n",
    "\n",
    "As the train set may include gaps in the numbering of either user_id or movie_id we use dictionaries to translate the movie_id (or user_id) to matrix_id\n",
    "\n",
    "For the learning part, we use the algorithm described on the slides from the lecture, we calculate the prediction as a scalar multiplication of the according vectors in matrixes of users and items. Then we calculate the error and update the vectors by learning_rate*(error*user_matrix_vector- lamb*self.item_matrix_vector). As we can see there are two parameters learning rate and lambda that we can tune.\n",
    "\n",
    "We initialize the matrixes with random numbers using np.random.rand function\n",
    "\n",
    "The output is put into [1:5] interval by simply setting every output < 1 into 1 and every output > 5 as 5, other outputs are not changed.\n",
    "\n",
    "In the testing function, when the test example uses user_id that did not appear in the train set (in case of a movie_id not appearing in the train set proceed similarly) we take the sum of the vectors for the corresponding movie_id and scale the sums of features of all movie_ids into a [1:5] interval, and read the value of the movie_id that is asked in the testing example (after the scaling). \n",
    "\n",
    "The assumption is that the bigger the sum of features the bigger the chance is for the movie to get a high score. This assumption is not 100% true, as for example having feature < 0 with a corresponding <0 feature in the user_id vector creates a positive result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization:\n",
    "    def __init__(self,x, num_features):\n",
    "        #initilaze two matrixes that then multiply by each other to give a matrix of ratings\n",
    "        \n",
    "        user_size = np.unique(x['user_id']).shape[0]\n",
    "        item_size = np.unique(x['Movie_Id']).shape[0]\n",
    "\n",
    "        values_user = np.unique(x['user_id'])\n",
    "        self.dict_user = {values_user[i] : i for i in range(len(values_user))}\n",
    "\n",
    "        values_item = np.unique(x['Movie_Id'])\n",
    "        self.dict_item = {values_item[i] : i for i in range(len(values_item))}\n",
    "        \n",
    "        self.user_matrix = np.random.rand(user_size,num_features)\n",
    "        self.item_matrix = np.random.rand(item_size,num_features)\n",
    "        \n",
    "    def fit(self,x, learning_rate = 0.005, lamb = 0.05, n_iter = 10):\n",
    "        for it in range(n_iter):\n",
    "            tmp = 0\n",
    "            for i in range(len(x)):\n",
    "                user = x.loc[i]['user_id']\n",
    "                item = x.loc[i]['Movie_Id']\n",
    "\n",
    "                user_idx = self.dict_user[user]\n",
    "                item_idx = self.dict_item[item]\n",
    "                \n",
    "                #calculate the error\n",
    "                error = x.loc[i]['Ratings'] - min(max(np.matmul(self.user_matrix[user_idx],self.item_matrix[item_idx]),1),5)\n",
    "                # update values\n",
    "                self.user_matrix[user_idx] = self.user_matrix[user_idx] + learning_rate*(error*self.item_matrix[item_idx] - lamb*self.user_matrix[user_idx])\n",
    "                self.item_matrix[item_idx] = self.item_matrix[item_idx] + learning_rate*(error*self.user_matrix[user_idx] - lamb*self.item_matrix[item_idx])\n",
    "\n",
    "                tmp += 1\n",
    "                if tmp%50000 ==0:\n",
    "                    print(f'currently done: {tmp/len(x)} % of the iteration {it}')\n",
    "\n",
    "        \n",
    "        print('current iteration ended: '+str(it))\n",
    "    def test(self,x):\n",
    "        predictions = []\n",
    "        for i in range(len(x)):\n",
    "            \n",
    "            user = x.loc[i]['user_id']\n",
    "            item = x.loc[i]['Movie_Id']\n",
    "\n",
    "            try:\n",
    "                user_idx = self.dict_user[user]\n",
    "                item_idx = self.dict_item[item]\n",
    "                pred = min(max(np.matmul(self.user_matrix[user_idx],self.item_matrix[item_idx]),1),5)\n",
    "                predictions.append(pred)\n",
    "                \n",
    "            except: #If there is no user\n",
    "                try:\n",
    "                    item_idx = self.dict_item[item]\n",
    "                    sum_item = np.sum(self.item_matrix[item_idx])\n",
    "                    sums = np.sum(self.item_matrix, axis = 1)\n",
    "                    pred =  (sum_item- np.min(sums)) / (np.max(sums) - np.min(sums)) * (4) + 1\n",
    "                    predictions.append(pred)\n",
    "                \n",
    "                except: # If there is no movie\n",
    "                    user_idx = self.dict_user[user]\n",
    "                    sum_user= np.sum(self.user_matrix[item_idx])\n",
    "                    sums = np.sum(self.user_matrix, axis = 1)\n",
    "                    pred =  (sum_user - np.min(sums)) / (np.max(sums) - np.min(sums)) * (4) + 1\n",
    "                    predictions.append(pred)\n",
    "                #calculate the error\n",
    "            \n",
    "        labels = np.array(x['Ratings'])\n",
    "        predictions = np.array(predictions)\n",
    "        rmse =  mean_squared_error(labels,predictions, squared = False)\n",
    "        mse = mean_absolute_error(labels,predictions)\n",
    "\n",
    "        return rmse, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion part one\n",
    "This concludes the first part of our report about recommender systems. For the result and data visualisation of our matrix factorization method, see part two of the report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
